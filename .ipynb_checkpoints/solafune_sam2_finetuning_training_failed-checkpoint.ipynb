{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0aecdb-7320-4245-af2c-cce1b78b3348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# Once the repo is cloned then:\n",
    "#best practice remove and reeinstal fspec\n",
    "#!rm -rf /opt/conda/lib/python3.10/site-packages/fsspec*\n",
    "#!pip install fsspec==2024.6.1 --force-reinstall --no-deps\n",
    "\n",
    "#pip install e .\n",
    "#pip install -e \".[demo]\"\n",
    "\n",
    "\n",
    "# #install also to vizualize figures\n",
    "# sudo apt-get update\n",
    "# sudo apt-get install -y libgl1-mesa-glx\n",
    "# sudo apt-get install -y libglib2.0-0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ae4c7-adf9-44bc-982c-af127338dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390f1661-54eb-488c-b620-62c3bef594ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fd8c9d-071f-4b9d-a914-ea50a845a3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rasterio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb576d66-c369-4fe4-a5a2-2bb011b16c26",
   "metadata": {},
   "source": [
    "## Running an example of general segmentation using SAM2\n",
    "\n",
    "In this example, the model segments everything it finds in the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f814815a-be3c-4d27-a639-4cad4db1f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d062fafb-4e8b-4ed6-a3f2-c508c7bfada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977acfa9-cdb8-4df1-95b8-04df8103b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "def show_anns(anns, borders=True):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:, :, 3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.5]])\n",
    "        img[m] = color_mask \n",
    "        if borders:\n",
    "            import cv2\n",
    "            contours, _ = cv2.findContours(m.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "            # Try to smooth contours\n",
    "            contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "            cv2.drawContours(img, contours, -1, (0, 0, 1, 0.4), thickness=1) \n",
    "\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee834ef-901a-4f8c-b03f-646ce546fe64",
   "metadata": {},
   "source": [
    "### Geting the image from Solafune competition uploaded to my s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12637dfb-26a4-4130-9c51-af00d539240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "def load_image_as_array(image_s3_uri):\n",
    "    \"\"\"Load an image from S3 and convert it to a NumPy array.\"\"\"\n",
    "    # Open the image using rasterio\n",
    "    with rasterio.open(image_s3_uri) as src:\n",
    "        # Read the image data\n",
    "        image = src.read()  # This will read all the bands\n",
    "        image = np.moveaxis(image, 0, -1)  # Move channels to the last dimension\n",
    "        image = image[:, :, :3]  # Assuming you want to use only the first 3 bands (R, G, B)\n",
    "    return image\n",
    "\n",
    "# Example usage\n",
    "image_s3_uri = 's3://solafune/train_images/images/train_25.tif'\n",
    "image = load_image_as_array(image_s3_uri)\n",
    "\n",
    "# Now 'image' is a NumPy array in RGB format\n",
    "print(image.shape)  # This will print the shape of the image array\n",
    "\n",
    "\n",
    "# Assuming 'image' is the array you loaded\n",
    "image_normalized = (image - np.min(image)) / (np.max(image) - np.min(image)) * 255\n",
    "image_normalized = image_normalized.astype(np.uint8)  # Convert to uint8\n",
    "\n",
    "image = image_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9dc7bd-668a-4b0e-a275-035e6a0bfe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "\n",
    "sam2_checkpoint = \"./segment-anything-2/checkpoints/sam2_hiera_large.pt\" # try large sam2_hiera_large.pt\n",
    "model_cfg = \"sam2_hiera_l.yaml\" #\"sam2_hiera_l.yaml\" for large\n",
    "\n",
    "sam2 = build_sam2(model_cfg, sam2_checkpoint, device=device, apply_postprocessing=False)\n",
    "\n",
    "mask_generator = SAM2AutomaticMaskGenerator(sam2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17adf67-884c-44c7-906d-349bb92b30d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = mask_generator.generate(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46734eae-0389-46e0-a8ea-275a8ce01c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(masks))\n",
    "print(masks[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b31053-0be0-4152-9d61-23f1885a2092",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(image)\n",
    "show_anns(masks)\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faea0729-136d-4155-8dfe-0d3286da720c",
   "metadata": {},
   "source": [
    "# Finetuning by retraining the model with solafune's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f0bf1d-0ce5-480b-ba7e-ea3bb9f3b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import transform\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def polygon_to_mask(polygon, width, height):\n",
    "    mask = Image.new('L', (width, height), 0)\n",
    "    ImageDraw.Draw(mask).polygon(polygon, outline=1, fill=1)\n",
    "    return torch.tensor(np.array(mask), dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9feaeb-ff9f-4542-84b7-1d2a395a9822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_s3_uri):\n",
    "    \"\"\"Load an image from S3 and convert it to a NumPy array.\"\"\"\n",
    "    # Open the image using rasterio\n",
    "    with rasterio.open(image_s3_uri) as src:\n",
    "        # Read the image data\n",
    "        image = src.read()  # This will read all the bands\n",
    "        image = np.moveaxis(image, 0, -1)  # Move channels to the last dimension\n",
    "        image = image[:, :, :3]  # Assuming you want to use only the first 3 bands (R, G, B)\n",
    "        image_normalized = (image - np.min(image)) / (np.max(image) - np.min(image)) * 255\n",
    "        image_normalized = image_normalized.astype(np.uint8)\n",
    "    return image_normalized\n",
    "\n",
    "\n",
    "def load_annotations(annotation_s3_uri, image_filename):\n",
    "    \"\"\"Load annotations for a specific image from a JSON file on S3.\"\"\"\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    with fs.open(annotation_s3_uri, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    for img in data['images']:\n",
    "        if img['file_name'] == image_filename:\n",
    "            return img['annotations']\n",
    "    return None\n",
    "    \n",
    "def load_original_annotations(annotation_s3_uri):\n",
    "    \"\"\"Load annotations for a specific image from a JSON file on S3.\"\"\"\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    with fs.open(annotation_s3_uri, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30004491-9ec6-4527-9b73-1148808bd09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SAM2Dataset(Dataset):\n",
    "    def __init__(self, image_filenames, annotations, image_s3_prefix):\n",
    "        self.image_filenames = image_filenames\n",
    "        self.annotations = annotations\n",
    "        self.image_s3_prefix = image_s3_prefix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.image_filenames[idx]\n",
    "        image_s3_uri = os.path.join(self.image_s3_prefix, image_filename)\n",
    "\n",
    "        # Load image using the new load_image function\n",
    "        image = load_image(image_s3_uri)\n",
    "        \n",
    "        # Ensure the image is in the correct format for the model\n",
    "        image_tensor = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "        \n",
    "        # Set requires_grad=True to enable gradient computation\n",
    "        #image_tensor.requires_grad_(True)\n",
    "\n",
    "        # Load annotations\n",
    "        annotations = load_annotations(self.annotations, image_filename)\n",
    "        masks = []\n",
    "        height, width = image.shape[:2]  # Get the image dimensions\n",
    "        for annotation in annotations:\n",
    "            mask = polygon_to_mask(annotation['segmentation'], width, height)\n",
    "            masks.append(mask)\n",
    "\n",
    "        # Stack all masks into a single tensor\n",
    "        masks_tensor = torch.stack(masks, dim=0)\n",
    "\n",
    "        return image_tensor, masks_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff306a2-8f45-4dae-9ab8-17814ebd3700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Json\n",
    "import s3fs\n",
    "import json\n",
    "train_annotation_s3_uri = 's3://solafune/train_annotation.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f192b6-c4a1-4f22-94f3-c37ba7730989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# List of image filenames\n",
    "image_filenames = [img['file_name'] for img in load_original_annotations(train_annotation_s3_uri)['images']]\n",
    "\n",
    "#image_filenames = ['train_28.tif'] # set a quick training with specific images\n",
    "image_s3_prefix = 's3://solafune/train_images/images'\n",
    "\n",
    "# Instantiate Dataset\n",
    "dataset = SAM2Dataset(image_filenames, train_annotation_s3_uri, image_s3_prefix)\n",
    "\n",
    "# Create DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f908abdb-b919-46ba-9dcc-61d0e190fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
    "\n",
    "model = build_sam2(model_cfg, sam2_checkpoint, device=device, apply_postprocessing=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17212463-9984-43bd-b09e-dfa2e41606ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary_masks(predicted_masks):\n",
    "    \"\"\"\n",
    "    Converts boolean masks from SAM2's output to binary masks.\n",
    "\n",
    "    Args:\n",
    "    - predicted_masks (list of dict): List of predicted masks with 'segmentation' key containing boolean arrays.\n",
    "\n",
    "    Returns:\n",
    "    - binary_masks (list of np.array): List of binary masks (1 and 0).\n",
    "    \"\"\"\n",
    "    binary_masks = []\n",
    "    for mask_data in predicted_masks:\n",
    "        # Convert the boolean segmentation mask to an integer binary mask\n",
    "        binary_mask = mask_data['segmentation'].astype(np.uint8)\n",
    "        binary_masks.append(binary_mask)\n",
    "    \n",
    "    return binary_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89993076-3e9b-45f8-992b-ad024460efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import json\n",
    "\n",
    "#Looking into the data - Checking if everything is in good order #  image and gt masks\n",
    "\n",
    "\n",
    "for i, (images, masks) in enumerate(data_loader):\n",
    "    print(f\"Batch {i + 1}:\")\n",
    "    print(f\" - Image shape before permute: {images.shape}\")\n",
    "    print(f\" - Masks shape: {masks.shape}\")\n",
    "    \n",
    "    # Assuming images are in the shape [B, C, H, W]\n",
    "    # Permute to shape [H, W, C] for visualization\n",
    "    image = images[0].permute(1, 2, 0)\n",
    "    print(f\" - Image shape after permute: {image.shape}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Show the image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image.detach().numpy())\n",
    "    plt.title(\"Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Show the masks overlayed on the image\n",
    "    combined_mask = masks[0].sum(axis=0)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(image.detach().numpy())\n",
    "    plt.imshow(combined_mask, alpha=0.5, cmap='jet')\n",
    "    plt.title(\"Masks Overlay\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Break after the first batch to inspect\n",
    "    if i == 0:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986edb13-16ca-4efd-b1ae-3fbdd1c7325b",
   "metadata": {},
   "source": [
    "# Main train Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4964c35d-ce64-4260-98da-36cb2e9008bd",
   "metadata": {},
   "source": [
    "##### Using a panoptic loss both to better perform on the chalange, and also to compare multiple masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4208a3e-4254-4438-a00d-df07319b67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PanopticLoss(nn.Module):\n",
    "    def __init__(self, iou_threshold=0.5):\n",
    "        super(PanopticLoss, self).__init__()\n",
    "        self.iou_threshold = iou_threshold\n",
    "\n",
    "    def forward(self, predicted_masks, target_masks):\n",
    "        # Convert to boolean tensors for logical operations\n",
    "        predicted_masks = predicted_masks > 0.5\n",
    "        target_masks = target_masks > 0.5\n",
    "\n",
    "        print(f\"Predicted Masks Shape: {predicted_masks.shape}\")\n",
    "        print(f\"Target Masks Shape: {target_masks.shape}\")\n",
    "\n",
    "        batch_size = predicted_masks.shape[0]\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            pred = predicted_masks[i]\n",
    "            target = target_masks[i]\n",
    "\n",
    "            print(f\"Batch {i + 1}: Pred Shape: {pred.shape}, Target Shape: {target.shape}\")\n",
    "\n",
    "            matched_pred, matched_target = self.match_masks(pred, target)\n",
    "\n",
    "            print(f\"Matched Pred Shape: {matched_pred.shape}, Matched Target Shape: {matched_target.shape}\")\n",
    "\n",
    "            iou_scores = self.calculate_iou(matched_pred, matched_target)\n",
    "\n",
    "            print(f\"IoU Scores: {iou_scores}\")\n",
    "\n",
    "            if iou_scores.numel() > 0:  # Ensure there's something to calculate\n",
    "                loss = 1 - iou_scores.mean()\n",
    "                print(f\"Loss for Batch {i + 1}: {loss.item()}\")\n",
    "            else:\n",
    "                loss = torch.tensor(0.0, device=pred.device)\n",
    "                print(\"No valid IoU scores, setting loss to 0.\")\n",
    "\n",
    "            total_loss += loss\n",
    "\n",
    "        final_loss = total_loss / batch_size\n",
    "        print(f\"Final Loss: {final_loss.item()}\")\n",
    "\n",
    "        return final_loss\n",
    "\n",
    "    def match_masks(self, pred, target):\n",
    "        pred = pred.view(pred.size(0), -1)\n",
    "        target = target.view(target.size(0), -1)\n",
    "\n",
    "        print(f\"Flattened Pred Shape: {pred.shape}\")\n",
    "        print(f\"Flattened Target Shape: {target.shape}\")\n",
    "\n",
    "        matched_pred = []\n",
    "        matched_target = []\n",
    "\n",
    "        for i, t in enumerate(target):\n",
    "            best_iou = 0\n",
    "            best_pred = None\n",
    "            for j, p in enumerate(pred):\n",
    "                intersection = torch.logical_and(p, t).float().sum()\n",
    "                union = torch.logical_or(p, t).float().sum()\n",
    "                iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_pred = p\n",
    "            if best_iou > self.iou_threshold:\n",
    "                matched_pred.append(best_pred)\n",
    "                matched_target.append(t)\n",
    "\n",
    "        if matched_pred and matched_target:\n",
    "            print(f\"Number of Matched Pairs: {len(matched_pred)}\")\n",
    "            return torch.stack(matched_pred), torch.stack(matched_target)\n",
    "        else:\n",
    "            print(\"No matches found. Returning zero-filled tensors.\")\n",
    "            return torch.zeros_like(pred), torch.zeros_like(target)\n",
    "\n",
    "    def calculate_iou(self, pred, target):\n",
    "        if pred.dim() == 2:\n",
    "            pred = pred.unsqueeze(0)\n",
    "        if target.dim() == 2:\n",
    "            target = target.unsqueeze(0)\n",
    "\n",
    "        intersection = (pred & target).float().sum((1, 2))\n",
    "        union = (pred | target).float().sum((1, 2))\n",
    "        iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "        return iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b1c7c8-e84e-4b24-9da8-49d040b2616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# class SAM2MaskGenerationFunction(torch.autograd.Function):\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, img_tensor, mask_generator):\n",
    "#         # Save the original image tensor for backward pass\n",
    "#         ctx.save_for_backward(img_tensor)\n",
    "        \n",
    "#         # Convert the image tensor to a NumPy array for SAM2\n",
    "#         img_np = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "#         # Generate masks using SAM2 (non-differentiable operation)\n",
    "#         masks_output = mask_generator.generate(img_np)\n",
    "\n",
    "#         # Convert masks to binary mask tensor\n",
    "#         binary_masks_np = np.array(convert_to_binary_masks(masks_output))\n",
    "#         binary_masks_tensor = torch.tensor(binary_masks_np, dtype=torch.float32).to(img_tensor.device)\n",
    "        \n",
    "#         # Reintroduce a differentiable operation\n",
    "#         #binary_masks_tensor = binary_masks_tensor * img_tensor.sum() * 0 + binary_masks_tensor\n",
    "\n",
    "#         return binary_masks_tensor\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx, grad_output):\n",
    "#         # Retrieve the saved image tensor\n",
    "#         img_tensor, = ctx.saved_tensors\n",
    "\n",
    "#         # Here, sum/average the gradients to reduce the dimensions\n",
    "#         # For simplicity, we assume reducing across the first dimension (e.g., channels or masks)\n",
    "#         grad_input = grad_output.sum(dim=0, keepdim=True)  # Adjust this based on how you want to reduce\n",
    "\n",
    "#         # Optionally, expand/reduce dimensions to match input tensor\n",
    "#         grad_input = grad_input.expand_as(img_tensor)\n",
    "\n",
    "#         return grad_input, None  # Return gradients for img_tensor and None for mask_generator\n",
    "\n",
    "# # Example usage remains the same as before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113931e-4d5e-4333-a85c-7e560f447e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleBCELoss, self).__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, predicted_masks, target_masks):\n",
    "        # Check initial grad_fn\n",
    "        print(\"Initial predicted_masks grad_fn:\", predicted_masks.grad_fn)\n",
    "        print(\"Initial target_masks grad_fn:\", target_masks.grad_fn)\n",
    "\n",
    "        # Collapse masks by summing across the mask dimension (dim=1)\n",
    "        predicted_mask_sum = predicted_masks.sum(dim=1)\n",
    "        target_mask_sum = target_masks.sum(dim=1)\n",
    "\n",
    "        # Flatten the masks to make them suitable for binary cross-entropy loss\n",
    "        predicted_mask_flat = predicted_mask_sum.view(predicted_mask_sum.size(0), -1)\n",
    "        target_mask_flat = (target_mask_sum > 0).float().view(target_mask_sum.size(0), -1)\n",
    "\n",
    "        # Calculate binary cross-entropy loss with logits\n",
    "        loss = self.bce_loss(predicted_mask_flat, target_mask_flat)\n",
    "\n",
    "        # Check if loss has a valid grad_fn\n",
    "        print(\"Loss grad_fn:\", loss.grad_fn)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601dbe9c-1cd2-456e-976b-dc0f4f39bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_feedback(model, data_loader, mask_generator, num_epochs=5, print_interval=1):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    #criterion = PanopticLoss(iou_threshold=0.5)  # Make sure this is fully differentiable\n",
    "    criterion = SimpleBCELoss()  # Make sure this is fully differentiableSimplifiedPanopticLoss\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for i, (images, masks) in enumerate(tqdm(data_loader, total=len(data_loader))):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Generate predicted masks\n",
    "            masks_pred = []\n",
    "            for img in images:\n",
    "                img = img.permute(1, 2, 0).cpu().numpy()\n",
    "                masks_output = mask_generator.generate(img)\n",
    "                binary_masks_np = np.array(convert_to_binary_masks(masks_output))\n",
    "                masks_pred.append(torch.tensor(binary_masks_np, dtype=torch.float32,requires_grad=True).to(device))\n",
    "\n",
    "            masks_pred = torch.stack(masks_pred)\n",
    "            \n",
    "            # Check if masks_pred still has a valid grad_fn\n",
    "            #print(\"Before loss calculation:\", masks_pred.grad_fn)\n",
    "            #loss = masks_pred.sum()  # Simplified loss for debugging\n",
    "            # Compute the loss (simplify this part to isolate the problem)\n",
    "            loss = criterion(masks_pred, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Print loss for every `print_interval` batches\n",
    "            if (i + 1) % print_interval == 0:\n",
    "                print(f\"Batch [{i + 1}/{len(data_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Average Loss: {epoch_loss / len(data_loader):.4f}\")\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86007140-84ab-492d-80b6-07efa16c2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual data loader, model, and mask_generator\n",
    "train_model_with_feedback(model, data_loader, mask_generator, num_epochs=5, print_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7fbd10-87ca-4940-9080-d7f3d20dbc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose `model` is your model instance\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08423562-b610-4938-b198-52492329658b",
   "metadata": {},
   "source": [
    "## Test the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a13523a-ee21-47ca-a436-997546689368",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()  # Set the model to evaluation mode if you're done with training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb28de7-bf78-46b6-9690-f554561108b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the new model\n",
    "\n",
    "image_s3_uri = 's3://solafune/train_images/images/train_0.tif'\n",
    "image = load_image_as_array(image_s3_uri)\n",
    "image_normalized = (image - np.min(image)) / (np.max(image) - np.min(image)) * 255\n",
    "image_normalized = image_normalized.astype(np.uint8)  # Convert to uint8\n",
    "image = image_normalized\n",
    "\n",
    "#building new generat\n",
    "\n",
    "\n",
    "new_mask_generator = SAM2AutomaticMaskGenerator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b06bc29-8c80-4a6f-804a-453dd7be8b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = mask_generator.generate(image)\n",
    "new_masks = new_mask_generator.generate(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242a2dfb-921f-4240-a6c5-82c89d16e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_anns(masks)\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c19289c-4633-4eeb-9411-eb5dd37bfe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "show_anns(new_masks)\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11edd78-5d11-48df-85f0-3c82c78fef72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e0dd0-d819-47d8-be2c-b714f1c2b3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
