{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "818d940c-23ae-45e5-976c-ee7f72089982",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'LabPicsV1//Simple/Train/Image/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m data_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabPicsV1//\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Path to dataset (LabPics 1)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m data\u001b[38;5;241m=\u001b[39m[] \u001b[38;5;66;03m# list of files in dataset\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ff, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSimple/Train/Image/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m):  \u001b[38;5;66;03m# go over all folder annotation\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m:data_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimple/Train/Image/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mname,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotation\u001b[39m\u001b[38;5;124m\"\u001b[39m:data_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimple/Train/Instance/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mname[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_batch\u001b[39m(data): \u001b[38;5;66;03m# read random image and its annotaion from  the dataset (LabPics)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m    \u001b[38;5;66;03m#  select image\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'LabPicsV1//Simple/Train/Image/'"
     ]
    }
   ],
   "source": [
    "# Train/Fine-Tune SAM 2 on the LabPics 1 dataset\n",
    "\n",
    "# Toturial: https://medium.com/@sagieppel/train-fine-tune-segment-anything-2-sam-2-in-60-lines-of-code-928dd29a63b3\n",
    "# Main repo: https://github.com/facebookresearch/segment-anything-2\n",
    "# Labpics Dataset can be downloaded from: https://zenodo.org/records/3697452/files/LabPicsV1.zip?download=1\n",
    "# Pretrained models for sam2 Can be downloaded from: https://github.com/facebookresearch/segment-anything-2?tab=readme-ov-file#download-checkpoints\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "# Read data\n",
    "\n",
    "data_dir=r\"LabPicsV1//\" # Path to dataset (LabPics 1)\n",
    "data=[] # list of files in dataset\n",
    "for ff, name in enumerate(os.listdir(data_dir+\"Simple/Train/Image/\")):  # go over all folder annotation\n",
    "    data.append({\"image\":data_dir+\"Simple/Train/Image/\"+name,\"annotation\":data_dir+\"Simple/Train/Instance/\"+name[:-4]+\".png\"})\n",
    "\n",
    "\n",
    "def read_batch(data): # read random image and its annotaion from  the dataset (LabPics)\n",
    "\n",
    "   #  select image\n",
    "\n",
    "        ent  = data[np.random.randint(len(data))] # choose random entry\n",
    "        Img = cv2.imread(ent[\"image\"])[...,::-1]  # read image\n",
    "        ann_map = cv2.imread(ent[\"annotation\"]) # read annotation\n",
    "\n",
    "   # resize image\n",
    "\n",
    "        r = np.min([1024 / Img.shape[1], 1024 / Img.shape[0]]) # scalling factor\n",
    "        Img = cv2.resize(Img, (int(Img.shape[1] * r), int(Img.shape[0] * r)))\n",
    "        ann_map = cv2.resize(ann_map, (int(ann_map.shape[1] * r), int(ann_map.shape[0] * r)),interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "   # merge vessels and materials annotations\n",
    "\n",
    "        mat_map = ann_map[:,:,0] # material annotation map\n",
    "        ves_map = ann_map[:,:,2] # vessel  annotaion map\n",
    "        mat_map[mat_map==0] = ves_map[mat_map==0]*(mat_map.max()+1) # merge maps\n",
    "\n",
    "   # Get binary masks and points\n",
    "\n",
    "        inds = np.unique(mat_map)[1:] # load all indices\n",
    "        points= []\n",
    "        masks = []\n",
    "        for ind in inds:\n",
    "            mask=(mat_map == ind).astype(np.uint8) # make binary mask corresponding to index ind\n",
    "            masks.append(mask)\n",
    "            coords = np.argwhere(mask > 0) # get all coordinates in mask\n",
    "            yx = np.array(coords[np.random.randint(len(coords))]) # choose random point/coordinate\n",
    "            points.append([[yx[1], yx[0]]])\n",
    "        return Img,np.array(masks),np.array(points), np.ones([len(masks),1])\n",
    "\n",
    "# Load model\n",
    "\n",
    "sam2_checkpoint = \"sam2_hiera_small.pt\" # path to model weight (pre model loaded from: https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt)\n",
    "model_cfg = \"sam2_hiera_s.yaml\" #  model config\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\") # load model\n",
    "predictor = SAM2ImagePredictor(sam2_model)\n",
    "\n",
    "# Set training parameters\n",
    "\n",
    "predictor.model.sam_mask_decoder.train(True) # enable training of mask decoder\n",
    "predictor.model.sam_prompt_encoder.train(True) # enable training of prompt encoder\n",
    "optimizer=torch.optim.AdamW(params=predictor.model.parameters(),lr=1e-5,weight_decay=4e-5)\n",
    "scaler = torch.cuda.amp.GradScaler() # mixed precision\n",
    "\n",
    "# Training loop\n",
    "\n",
    "for itr in range(100000):\n",
    "    with torch.cuda.amp.autocast(): # cast to mix precision\n",
    "            image,mask,input_point, input_label = read_batch(data) # load data batch\n",
    "            if mask.shape[0]==0: continue # ignore empty batches\n",
    "            predictor.set_image(image) # apply SAM image encoder to the image\n",
    "\n",
    "            # prompt encoding\n",
    "\n",
    "            mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(input_point, input_label, box=None, mask_logits=None, normalize_coords=True)\n",
    "            sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(points=(unnorm_coords, labels),boxes=None,masks=None,)\n",
    "\n",
    "            # mask decoder\n",
    "\n",
    "            batched_mode = unnorm_coords.shape[0] > 1 # multi object prediction\n",
    "            high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n",
    "            low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),sparse_prompt_embeddings=sparse_embeddings,dense_prompt_embeddings=dense_embeddings,multimask_output=True,repeat_image=batched_mode,high_res_features=high_res_features,)\n",
    "            prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])# Upscale the masks to the original image resolution\n",
    "\n",
    "            # Segmentaion Loss caclulation\n",
    "\n",
    "            gt_mask = torch.tensor(mask.astype(np.float32)).cuda()\n",
    "            prd_mask = torch.sigmoid(prd_masks[:, 0])# Turn logit map to probability map\n",
    "            seg_loss = (-gt_mask * torch.log(prd_mask + 0.00001) - (1 - gt_mask) * torch.log((1 - prd_mask) + 0.00001)).mean() # cross entropy loss\n",
    "\n",
    "            # Score loss calculation (intersection over union) IOU\n",
    "\n",
    "            inter = (gt_mask * (prd_mask > 0.5)).sum(1).sum(1)\n",
    "            iou = inter / (gt_mask.sum(1).sum(1) + (prd_mask > 0.5).sum(1).sum(1) - inter)\n",
    "            score_loss = torch.abs(prd_scores[:, 0] - iou).mean()\n",
    "            loss=seg_loss+score_loss*0.05  # mix losses\n",
    "\n",
    "            # apply back propogation\n",
    "\n",
    "            predictor.model.zero_grad() # empty gradient\n",
    "            scaler.scale(loss).backward()  # Backpropogate\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update() # Mix precision\n",
    "\n",
    "            if itr%1000==0: torch.save(predictor.model.state_dict(), \"model.torch\");print(\"save model\")\n",
    "\n",
    "            # Display results\n",
    "\n",
    "            if itr==0: mean_iou=0\n",
    "            mean_iou = mean_iou * 0.99 + 0.01 * np.mean(iou.cpu().detach().numpy())\n",
    "            print(\"step)\",itr, \"Accuracy(IOU)=\",mean_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c92a7a8-3f67-4282-bb45-d52c6ba82d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
